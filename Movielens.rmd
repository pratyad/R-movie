---
title: "Movielens Capstone Project"
author: "Pratibha Yadav"
date: "5/12/2021"
output: pdf_document
---


# Summary

The objective of this project is to create a movie recommendation method using MovieLens data set. 
The variant of movielens data set used for this final project contains approximately 10 Millions of movies ratings, divided in 90% for training and 10% for validation. It is a small subset of a bigger data set.The training data set has 69878 users, 10677 movies and 797 genres combinations such as Action, Adventure, Horror, Crime, Thriller and more.
After a preliminary data study, the recommendation systems built on this data set are evaluated and selected based on the RMSE - Root Mean Squared Error. 

RMSE compute root mean square error (RMSE)
```{r}
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}
```

# Data Analysis


```{r, echo=FALSE}
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(data.table)
library(dplyr, warn.conflicts = FALSE)

# Suppress summarise info
options(dplyr.summarise.inform = FALSE)
dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
```

## Movielens data set

```{r}
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))
movielens <- left_join(ratings, movies, by = "movieId")
```

## Validation set will be 10% of MovieLens data

```{r}
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]
```

## Make sure userId and movieId in validation set are also in edx set

```{r}
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")
```

## Make sure userId and movieId in validation set are also in edx set

```{r}
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")
```

## Add rows removed from validation set back into edx set

```{r}
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)
rm(dl, ratings, movies, test_index, temp, movielens, removed)
```

# Structure of `edx`

```{r}
head(edx)
glimpse(edx)
```

How many distinct movie, users and genres

```{r}
n_distinct(edx$userId)
n_distinct(edx$genres)
n_distinct(edx$movieId)
nrow(edx)
```

# Histogram of edx 
Distribution of Movie Ratings

```{r}
edx %>% group_by(movieId) %>% summarize(n = n()) %>%
  ggplot(aes(n)) + geom_histogram(fill = "aliceblue", color = "black", bins = 10) +
  scale_x_log10() +
  ggtitle("Number of Movies Ratings")
```

Distribution of Users

```{r}
edx %>% group_by(userId) %>% summarize(n = n()) %>%
  ggplot(aes(n)) + geom_histogram(fill = "aliceblue", color = "grey", bins = 10) +
  scale_x_log10() + 
  ggtitle("Number of Users Ratings")
```

#Division of the edx set to create 2 sets for training and testing purposes.
training set is 80% and testing set is 20% of the edx set

```{r}
set.seed(1)
y <- edx$rating
test_index1 <- createDataPartition(y, times = 1, p = 0.2, list = FALSE)

train_set <- edx %>% slice(-test_index1)
test_set_temp <- edx %>% slice(test_index1)
```

Make sure userId and movieId in test set are also in train set

```{r}
test_set <- test_set_temp %>% 
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId")
```

Add rows removed from test set back into train set

```{r}
removed <- anti_join(test_set_temp, test_set)

train_set<-rbind(train_set, removed)
```


# Systems and Data Analysis

Calculate mean of ratings in training set and then calculate rmse
As we will be comparing different approaches, we create a results table with this naive approach:
```{r}
mean1<-mean(train_set$rating)
mean1
rmse_1 <- RMSE(test_set$rating, mean1) # compute root mean square error

rmse_1
rmse_results <- bind_rows(tibble(method="Average",  
                                 RMSE = rmse_1 ))
```

# Genres Effect
Do Genres have an effect on ratings? 
I extracted the genres from the data with the idea to do an analysis on each genre

```{r}
dat <- train_set %>% separate_rows(genres, sep ="\\|")

head(dat)

movie_avgs_genres<-dat%>%
  group_by(genres)%>%
  summarise(b_g= mean(rating-mean1))

dat_test<- test_set%>% separate_rows(genres, sep ="\\|")

predicted_ratings <- mean1 + dat_test %>% 
  left_join(movie_avgs_genres, by='genres') %>%
  .$b_g
```

calculate rmse using predicted ratings considering each genre

```{r}
rmse_genre <- RMSE(predicted_ratings, dat_test$rating)
rmse_genre
rmse_results <- bind_rows(rmse_results, tibble(method="RMSE with genre",  
                                 RMSE = rmse_genre ))
rmse_results
```

RMSE is still very high so we can conclude that genre does not lower RMSE considerably

# Modeling Movie Effects

calculate the average rating for movie i

```{r}
movie_avgs <- train_set %>% 
  group_by(movieId) %>% 
  summarize(b_i = mean(rating - mean1))
```

 plotting histogram of movie avg
 
```{r}
movie_avgs%>%ggplot(aes(b_i)) + geom_histogram(fill = "aliceblue",bins = 10, color = "black")
```

calculating predicted ratings for individual movie

```{r}
predicted_ratings <- mean1 + test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  .$b_i
```

calculate rmse using predicted ratings considering each movie

```{r}
rmse_2 <- RMSE(predicted_ratings, test_set$rating)
rmse_2
rmse_results<-bind_rows(rmse_results, tibble(method = "movie effect", RMSE= rmse_2))
```

There is an improvement in lowering rmse.We will now compute rmse based on effect of user on movie ratings

# Modeling User Effects

calculate the average rating for user u also plot it using histogram

```{r}
train_set %>% 
  group_by(userId) %>% 
  summarize(b_u = mean(rating - mean1)) %>% 
  ggplot(aes(b_u)) + 
  geom_histogram(fill = "aliceblue",bins = 20, color = "black")
```

calculate user effect b_u

```{r}
user_avgs <- train_set%>% 
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mean1 - b_i))
```

calculate predicted values on test set

```{r}
predicted_ratings <- test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(pred = mean1 + b_i + b_u) %>%
  .$pred
```

calculate rmse with user effect

```{r}
rmse_3 <- RMSE(predicted_ratings, test_set$rating)
rmse_3
rmse_results_final<-bind_rows(rmse_results, tibble(method = "user effect", RMSE = rmse_3))                                  
rmse_results_final
```

Improvement in rmse is approx 6% after considering individual movie effect and user effect.There are lots of movies which are rated very few times and based on those few ratings the calculation of bi and bu is either high or low.We will find out using below code mistakes in user effect model,

Find top 10 worst and best movies based on user effect
we will have to create a table with movie ID and title and then extract distinct titles
```{r}
movie_titles <- train_set %>% 
  select(movieId, title) %>%
  distinct()
```

Find top ten best movies using user effect
```{r}
movie_avgs %>% left_join(movie_titles, by="movieId") %>%
  arrange(desc(b_i)) %>% 
  select(title, b_i) %>% 
  slice(1:10) %>%  
 tibble()
```

Top 10 worse movies based on user effect on ratings
```{r}
movie_avgs %>% left_join(movie_titles, by="movieId") %>%
  arrange(b_i) %>% 
  select(title, b_i) %>% 
  slice(1:10) %>%  
  tibble()
```

All the movies in above two lists are not very famous movies.We will have to check how many times they were rated to see if there is a penalty of using small sample size in estimating ratings
Regularization helps in reducing the overfitting of training data set in return for a small amount of bias

Counting number of rating of the best movies by user effects on ratings

```{r}
train_set %>% dplyr::count(movieId) %>% 
  left_join(movie_avgs, by="movieId") %>%
  left_join(movie_titles, by="movieId") %>%
  arrange(desc(b_i)) %>% 
  slice(1:10) %>% 
  pull(n)
```

Counting number of rating of the worst movies by user effects on ratings

```{r}
train_set %>% dplyr::count(movieId) %>% 
  left_join(movie_avgs, by="movieId") %>%
  left_join(movie_titles, by="movieId") %>%
  arrange(b_i) %>% 
  slice(1:10) %>% 
  pull(n)
```

The best and worst movies were rated by few users,in most cases it is just 1. We need small value for parameter and a simpler hypothesis which will be less prone to overfitting. 

Choosing the penalty terms

We use regularization to estimate both movie and user effects. 
We can use cross-validation to pick regularization parameter or tuning parameter:

```{r}
lambda<-seq(0, 10, 0.25)
rmses<-sapply(lambda, function(l){
  mean_1<-mean(train_set$rating)
  b_i<-train_set%>%
    group_by(movieId)%>%
    summarise(b_i=sum(rating-mean1)/(n()+l))
  b_u<-train_set%>%
    left_join(b_i, by = "movieId")%>%
    group_by(userId)%>%
    summarise(b_u = sum(rating-mean1-b_i)/(n() + l))
  
   predicted_ratings<-train_set%>%
     left_join(b_i, by = "movieId") %>%
     left_join(b_u, by = "userId") %>%
     mutate(pred = mean1 + b_i + b_u) %>%
     .$pred
   
   return(RMSE(train_set$rating, predicted_ratings))
})
```

plot lambda and rmses

```{r}
qplot(lambda, rmses)
```

find value of lambda for which rmses is minimum

```{r}
lambda<-lambda[which.min(rmses)]
```

lambda is `r lambda`

Use lambda to calculate movie effect with regularization on training set

```{r}
b_i <- train_set %>% 
  group_by(movieId) %>%
  summarize(b_i = sum(rating - mean1)/(n()+lambda))
```

Calculate user effect with regularization

```{r}
b_u<-train_set%>%
  left_join(b_i, by ="movieId")%>%
  group_by(userId)%>%
  summarise(b_u= sum(rating-mean1-b_i)/(n() + lambda))
```

Calculate predicted rating using test_set and rmse

```{r}
predicted_ratings<-test_set%>%
  left_join(b_i, by ="movieId")%>%
  left_join(b_u, by = "userId")%>%
  mutate(pred= mean1 +b_i +b_u)%>%
  pull(pred)
 rmse_4 <-RMSE(test_set$rating, predicted_ratings)
rmse_4 
rmse_results_final<-bind_rows(rmse_results_final, tibble(method = "Regularization  and user effect", RMSE = rmse_4))                                  
rmse_results_final<-rmse_results_final[-4,]
```

RMSE final is  `r rmse_results_final`

#Regularization on validation set

```{r}
  mean1 <- mean(edx$rating)
  b_i <- edx %>%
    group_by(movieId) %>%
    summarise(b_i = sum(rating - mean1) / (n()+lambda))
  
  b_u <- edx %>%
    left_join(b_i, by = "movieId") %>%
    group_by(userId) %>%
    summarise(b_u = sum(rating - b_i - mean1) / (n()+lambda))
  
  predicted_ratings <- validation %>%
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(pred = mean1 + b_i + b_u) %>%
    pull(pred)
  
rmse_5<-RMSE(validation$rating, predicted_ratings)
rmse_5
```


#RMSE using validation set

```{r}
rmse_results_final<-bind_rows(rmse_results_final, tibble(method = "Regularization and user effect validation set", RMSE = rmse_5))                                  

rmse_results_final
```


This is the summary results for all the model built and trained on ``edx`` data set and validated on the ``validation`` data set
After training different models, we find that regularization along with movie and user effect lower RMSE to 0.86. It is true for both test set  from edx and validation set.
